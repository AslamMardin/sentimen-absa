{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac741594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Libraries untuk NLP dan Text Processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# Libraries untuk Topic Modeling dan Machine Learning\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Libraries untuk Transformers (mT5)\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "\n",
    "# Download NLTK data yang diperlukan\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"=== ANALISIS SENTIMEN DAN ASPEK ULASAN PESANTREN ===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4f5a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 1. LOADING DATA\n",
    "# =======================\n",
    "print(\"1. Loading Data...\")\n",
    "\n",
    "try:\n",
    "    # Load dataset utama\n",
    "    df = pd.read_csv('hasil_sentimen_pesantren.csv')\n",
    "    print(f\"Dataset utama berhasil dimuat: {df.shape}\")\n",
    "    print(f\"============text combined===========\")\n",
    "    print(df['text_combined'])\n",
    "    \n",
    "    # Load lexicon kata positif dan negatif\n",
    "    positive_words = pd.read_csv('positive.csv')['word'].tolist()\n",
    "    negative_words = pd.read_csv('negative.csv')['word'].tolist()\n",
    "    print(f\"Kata positif: {len(positive_words)}, Kata negatif: {len(negative_words)}\")\n",
    "    \n",
    "    # Display info dataset\n",
    "    print(\"\\nInfo Dataset:\")\n",
    "    print(df.info())\n",
    "    print(\"\\nSample data:\")\n",
    "    print(df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Pastikan file CSV tersedia dan nama kolom sesuai\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d7a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 2. TEXT PREPROCESSING\n",
    "# =======================\n",
    "print(\"\\n2. Text Preprocessing...\")\n",
    "\n",
    "# Inisialisasi tools preprocessing bahasa Indonesia\n",
    "factory = StopWordRemoverFactory()\n",
    "stopword_remover = factory.create_stop_word_remover()\n",
    "\n",
    "stemmer_factory = StemmerFactory()\n",
    "stemmer = stemmer_factory.create_stemmer()\n",
    "\n",
    "# Tambahan stopwords bahasa Indonesia\n",
    "indonesian_stopwords = set(stopwords.words('indonesian'))\n",
    "custom_stopwords = {'yang', 'ini', 'itu', 'dengan', 'untuk', 'pada', 'dalam', 'dari', 'ke', 'di', 'dan', 'atau', 'adalah', 'akan', 'sudah', 'telah', 'dapat', 'bisa', 'harus', 'juga', 'saja', 'hanya', 'sangat', 'lebih', 'paling', 'agak', 'cukup'}\n",
    "all_stopwords = indonesian_stopwords.union(custom_stopwords)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Membersihkan teks dari karakter tidak perlu\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove URLs, mentions, hashtags\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_text(text, remove_stopwords=True, do_stemming=False):\n",
    "    \"\"\"Preprocessing teks lengkap\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # Clean text\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        tokens = [word for word in tokens if word not in all_stopwords and len(word) > 2]\n",
    "    \n",
    "    # Stemming (optional)\n",
    "    if do_stemming:\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Applying text preprocessing...\")\n",
    "df['text_cleaned'] = df['text_combined'].apply(lambda x: preprocess_text(x, remove_stopwords=False))\n",
    "df['text_processed'] = df['text_combined'].apply(lambda x: preprocess_text(x, remove_stopwords=True))\n",
    "\n",
    "print(\"Text preprocessing selesai!\")\n",
    "print(\"====================\")\n",
    "print(df[\"text_processed\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e960225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Text Refinement menggunakan T5 Bahasa Indonesia...\n",
      "Loading T5 model...\n",
      "Error loading model: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Menggunakan text cleaning sederhana sebagai alternatif\n",
      "Melakukan text refinement...\n",
      "Dataset besar terdeteksi. Memproses sample 100 data pertama untuk demo...\n",
      "Processing batch 1/10\n",
      "Processing batch 2/10\n",
      "Processing batch 3/10\n",
      "Processing batch 4/10\n",
      "Processing batch 5/10\n",
      "Processing batch 6/10\n",
      "Processing batch 7/10\n",
      "Processing batch 8/10\n",
      "Processing batch 9/10\n",
      "Processing batch 10/10\n",
      "Text refinement selesai!\n",
      "===========================\n",
      "                                         text_cleaned  \\\n",
      "0   masyaallah santri ppm al ikhlash serah bantu k...   \n",
      "1   pimpin pondok keren pimpin al ikhlash terima k...   \n",
      "2   sehat selalu pak ustaz aamiin pimpin al ikhlas...   \n",
      "3   semangat anak solehnya ummi d mudah nak d lanc...   \n",
      "4   masyaallah alhamdulillah ananda muh dzaki as s...   \n",
      "..                                                ...   \n",
      "95  kalau nur khalish sdh ikut tanding nak kelas x...   \n",
      "96  titah klas x tanding pencak silat antar putri ...   \n",
      "97  titah ramadhani tutu tanding pencak silat anta...   \n",
      "98  semangat mudah juara tanding pencak silat anta...   \n",
      "99  muhammad nur khalis kelas tanding pencak silat...   \n",
      "\n",
      "                                         text_refined  \n",
      "0   Masyaallah santri ppm al ikhlash serah bantu k...  \n",
      "1   Pimpin pondok keren pimpin al ikhlash terima k...  \n",
      "2   Sehat selalu pak ustaz aamiin pimpin al ikhlas...  \n",
      "3   Semangat anak solehnya ummi d mudah nak d lanc...  \n",
      "4   Masyaallah alhamdulillah ananda muh dzaki as s...  \n",
      "..                                                ...  \n",
      "95  Kalau nur khalish sdh ikut tanding nak kelas x...  \n",
      "96  Titah klas x tanding pencak silat antar putri ...  \n",
      "97  Titah ramadhani tutu tanding pencak silat anta...  \n",
      "98  Semangat mudah juara tanding pencak silat anta...  \n",
      "99  Muhammad nur khalis kelas tanding pencak silat...  \n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 3. TEXT REFINEMENT dengan mT5\n",
    "# =======================\n",
    "print(\"\\n3. Text Refinement menggunakan mT5...\")\n",
    "\n",
    "class TextRefiner:\n",
    "    def __init__(self):\n",
    "        self.model_name = \"cahya/t5-base-indonesian-summarization-cased\"\n",
    "        try:\n",
    "            print(\"Loading mT5 model...\")\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(self.model_name)\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(self.model_name)\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            self.model.to(self.device)\n",
    "            print(f\"Model loaded successfully on {self.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            print(\"Menggunakan text cleaning sederhana sebagai alternatif\")\n",
    "            self.model = None\n",
    "    \n",
    "    def refine_text(self, text, max_length=512):\n",
    "        \"\"\"Memperbaiki kalimat menggunakan mT5\"\"\"\n",
    "        if self.model is None:\n",
    "            # Fallback: simple text improvement\n",
    "            return self.simple_text_improvement(text)\n",
    "        \n",
    "        if pd.isna(text) or text == \"\":\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            # Prepare input for paraphrasing\n",
    "            input_text = f\"paraphrase: {text}\"\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = self.tokenizer.encode(\n",
    "                input_text, \n",
    "                return_tensors=\"pt\", \n",
    "                max_length=max_length, \n",
    "                truncation=True\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Generate paraphrase\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    inputs,\n",
    "                    max_length=max_length,\n",
    "                    num_beams=4,\n",
    "                    early_stopping=True,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True\n",
    "                )\n",
    "            \n",
    "            # Decode output\n",
    "            refined_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            return refined_text if refined_text != \"\" else text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error refining text: {e}\")\n",
    "            return self.simple_text_improvement(text)\n",
    "    \n",
    "    def simple_text_improvement(self, text):\n",
    "        \"\"\"Perbaikan teks sederhana sebagai fallback\"\"\"\n",
    "        if pd.isna(text) or text == \"\":\n",
    "            return \"\"\n",
    "        \n",
    "        # Basic sentence structure improvement\n",
    "        text = str(text).strip()\n",
    "        \n",
    "        # Capitalize first letter\n",
    "        if text:\n",
    "            text = text[0].upper() + text[1:]\n",
    "        \n",
    "        # Ensure proper sentence ending\n",
    "        if text and text[-1] not in '.!?':\n",
    "            text += '.'\n",
    "        \n",
    "        # Fix common spacing issues\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'\\s+([.!?])', r'\\1', text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "# Initialize text refiner\n",
    "refiner = TextRefiner()\n",
    "\n",
    "# Apply text refinement (proses ini mungkin memakan waktu)\n",
    "print(\"Melakukan text refinement...\")\n",
    "if len(df) > 100:\n",
    "    print(\"Dataset besar terdeteksi. Memproses sample 100 data pertama untuk demo...\")\n",
    "    sample_df = df.head(100).copy()\n",
    "else:\n",
    "    sample_df = df.copy()\n",
    "\n",
    "# Batch processing untuk efisiensi\n",
    "batch_size = 10\n",
    "refined_texts = []\n",
    "\n",
    "for i in range(0, len(sample_df), batch_size):\n",
    "    batch = sample_df.iloc[i:i+batch_size]\n",
    "    print(f\"Processing batch {i//batch_size + 1}/{(len(sample_df)-1)//batch_size + 1}\")\n",
    "    \n",
    "    for text in batch['text_cleaned']:\n",
    "        refined_text = refiner.refine_text(text)\n",
    "        refined_texts.append(refined_text)\n",
    "\n",
    "sample_df['text_refined'] = refined_texts\n",
    "print(\"Text refinement selesai!\")\n",
    "print(\"===========================\")\n",
    "print(sample_df[\"text_refined\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f881494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 4. TOPIC MODELING dengan LDA\n",
    "# =======================\n",
    "print(\"\\n4. Topic Modeling dengan LDA...\")\n",
    "\n",
    "# Definisi 10 aspek\n",
    "aspect_labels = {\n",
    "    0: \"Kualitas Guru\",\n",
    "    1: \"Fasilitas\", \n",
    "    2: \"Lingkungan\",\n",
    "    3: \"Kegiatan Pondok\",\n",
    "    4: \"Pembinaan Karakter\",\n",
    "    5: \"Prestasi\",\n",
    "    6: \"Akademik\",\n",
    "    7: \"Motivasi/Spiritual\",\n",
    "    8: \"Sosial\",\n",
    "    9: \"Umum\"\n",
    "}\n",
    "\n",
    "def prepare_lda_data(texts):\n",
    "    \"\"\"Mempersiapkan data untuk LDA\"\"\"\n",
    "    # Tokenize and remove stopwords\n",
    "    processed_texts = []\n",
    "    for text in texts:\n",
    "        if pd.notna(text) and text != \"\":\n",
    "            # Simple preprocessing for LDA\n",
    "            tokens = simple_preprocess(text, deacc=True, min_len=3)\n",
    "            tokens = [token for token in tokens if token not in all_stopwords]\n",
    "            if len(tokens) > 3:  # Only include documents with sufficient tokens\n",
    "                processed_texts.append(tokens)\n",
    "    \n",
    "    return processed_texts\n",
    "\n",
    "# Prepare data untuk LDA\n",
    "print(\"Preparing data for LDA...\")\n",
    "lda_texts = prepare_lda_data(sample_df['text_refined'])\n",
    "\n",
    "if len(lda_texts) > 0:\n",
    "    # Create dictionary and corpus\n",
    "    dictionary = corpora.Dictionary(lda_texts)\n",
    "    dictionary.filter_extremes(no_below=2, no_above=0.8)\n",
    "    corpus = [dictionary.doc2bow(text) for text in lda_texts]\n",
    "    \n",
    "    print(f\"Dictionary size: {len(dictionary)}\")\n",
    "    print(f\"Corpus size: {len(corpus)}\")\n",
    "    \n",
    "    # Train LDA model\n",
    "    print(\"Training LDA model...\")\n",
    "    num_topics = 10\n",
    "    lda_model = models.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics,\n",
    "        random_state=42,\n",
    "        passes=10,\n",
    "        alpha='auto',\n",
    "        per_word_topics=True,\n",
    "        minimum_probability=0.01\n",
    "    )\n",
    "    \n",
    "    print(\"LDA model training selesai!\")\n",
    "    \n",
    "    # Get topic predictions\n",
    "    def get_dominant_topic(text):\n",
    "        \"\"\"Mendapatkan topik dominan dari teks\"\"\"\n",
    "        if pd.isna(text) or text == \"\":\n",
    "            return 9  # Default to \"Umum\"\n",
    "        \n",
    "        tokens = simple_preprocess(text, deacc=True, min_len=3)\n",
    "        tokens = [token for token in tokens if token not in all_stopwords]\n",
    "        \n",
    "        if len(tokens) == 0:\n",
    "            return 9\n",
    "        \n",
    "        bow = dictionary.doc2bow(tokens)\n",
    "        if len(bow) == 0:\n",
    "            return 9\n",
    "        \n",
    "        topic_probs = lda_model.get_document_topics(bow)\n",
    "        if len(topic_probs) == 0:\n",
    "            return 9\n",
    "        \n",
    "        # Get topic with highest probability\n",
    "        dominant_topic = max(topic_probs, key=lambda x: x[1])[0]\n",
    "        return dominant_topic\n",
    "    \n",
    "    # Apply topic prediction\n",
    "    print(\"Predicting aspects...\")\n",
    "    sample_df['aspek_predicted'] = sample_df['text_refined'].apply(get_dominant_topic)\n",
    "    \n",
    "    # Display topic keywords\n",
    "    print(\"\\nTop keywords per topic:\")\n",
    "    for idx, topic in lda_model.print_topics(num_words=10):\n",
    "        print(f\"Topic {idx} ({aspect_labels[idx]}): {topic}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Tidak cukup data untuk training LDA. Menggunakan random assignment.\")\n",
    "    sample_df['aspek_predicted'] = np.random.randint(0, 10, len(sample_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd86e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 5. SENTIMENT ANALYSIS (Lexicon-based)\n",
    "# =======================\n",
    "print(\"\\n5. Sentiment Analysis (Lexicon-based)...\")\n",
    "\n",
    "def analyze_sentiment_lexicon(text, positive_words, negative_words):\n",
    "    \"\"\"Analisis sentimen berbasis lexicon\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return 'netral'\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Count positive and negative words\n",
    "    pos_count = sum(1 for word in tokens if word in positive_words)\n",
    "    neg_count = sum(1 for word in tokens if word in negative_words)\n",
    "    \n",
    "    # Determine sentiment\n",
    "    if pos_count > neg_count:\n",
    "        return 'positif'\n",
    "    elif neg_count > pos_count:\n",
    "        return 'negatif'\n",
    "    else:\n",
    "        return 'netral'\n",
    "\n",
    "# Apply sentiment analysis\n",
    "print(\"Analyzing sentiment...\")\n",
    "sample_df['sentimen_predicted'] = sample_df['text_refined'].apply(\n",
    "    lambda x: analyze_sentiment_lexicon(x, positive_words, negative_words)\n",
    ")\n",
    "\n",
    "print(\"Sentiment analysis selesai!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b5c06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 6. EVALUASI MODEL\n",
    "# =======================\n",
    "print(\"\\n6. Evaluasi Model...\")\n",
    "\n",
    "# Pastikan sample_df memiliki kolom yang dibutuhkan untuk evaluasi\n",
    "if 'aspek_manual' in sample_df.columns and 'sentimen_lexicon' in sample_df.columns:\n",
    "    \n",
    "    # Evaluasi Aspek\n",
    "    print(\"=== EVALUASI ASPEK ===\")\n",
    "    \n",
    "    # Convert aspek manual ke numeric jika berupa string\n",
    "    if sample_df['aspek_manual'].dtype == 'object':\n",
    "        le_aspect = LabelEncoder()\n",
    "        sample_df['aspek_manual_encoded'] = le_aspect.fit_transform(sample_df['aspek_manual'].astype(str))\n",
    "    else:\n",
    "        sample_df['aspek_manual_encoded'] = sample_df['aspek_manual']\n",
    "    \n",
    "    # Hitung akurasi aspek\n",
    "    aspect_accuracy = accuracy_score(sample_df['aspek_manual_encoded'], sample_df['aspek_predicted'])\n",
    "    print(f\"Akurasi Prediksi Aspek: {aspect_accuracy:.4f}\")\n",
    "    \n",
    "    # Confusion matrix aspek\n",
    "    aspect_cm = confusion_matrix(sample_df['aspek_manual_encoded'], sample_df['aspek_predicted'])\n",
    "    \n",
    "    # Classification report aspek\n",
    "    aspect_report = classification_report(\n",
    "        sample_df['aspek_manual_encoded'], \n",
    "        sample_df['aspek_predicted'],\n",
    "        target_names=[aspect_labels[i] for i in range(10)],\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(\"Classification Report - Aspek:\")\n",
    "    print(aspect_report)\n",
    "    \n",
    "    # Evaluasi Sentimen\n",
    "    print(\"\\n=== EVALUASI SENTIMEN ===\")\n",
    "    \n",
    "    # Hitung akurasi sentimen\n",
    "    sentiment_accuracy = accuracy_score(sample_df['sentimen_lexicon'], sample_df['sentimen_predicted'])\n",
    "    print(f\"Akurasi Prediksi Sentimen: {sentiment_accuracy:.4f}\")\n",
    "    \n",
    "    # Confusion matrix sentimen\n",
    "    sentiment_cm = confusion_matrix(sample_df['sentimen_lexicon'], sample_df['sentimen_predicted'])\n",
    "    \n",
    "    # Classification report sentimen\n",
    "    sentiment_report = classification_report(\n",
    "        sample_df['sentimen_lexicon'], \n",
    "        sample_df['sentimen_predicted'],\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(\"Classification Report - Sentimen:\")\n",
    "    print(sentiment_report)\n",
    "    \n",
    "else:\n",
    "    print(\"Kolom aspek_manual atau sentimen_lexicon tidak ditemukan. Melewati evaluasi.\")\n",
    "    aspect_accuracy = 0\n",
    "    sentiment_accuracy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7407d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 7. VISUALISASI\n",
    "# =======================\n",
    "print(\"\\n7. Membuat Visualisasi...\")\n",
    "\n",
    "# Set style untuk plot\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Create subplots\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# 1. Distribusi Aspek\n",
    "plt.subplot(3, 3, 1)\n",
    "aspect_counts = sample_df['aspek_predicted'].value_counts().sort_index()\n",
    "aspect_names = [aspect_labels[i] for i in aspect_counts.index]\n",
    "plt.bar(range(len(aspect_counts)), aspect_counts.values, color='skyblue')\n",
    "plt.title('Distribusi Aspek (Prediksi)', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Aspek')\n",
    "plt.ylabel('Jumlah')\n",
    "plt.xticks(range(len(aspect_counts)), aspect_names, rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# 2. Distribusi Sentimen\n",
    "plt.subplot(3, 3, 2)\n",
    "sentiment_counts = sample_df['sentimen_predicted'].value_counts()\n",
    "colors = {'positif': 'green', 'negatif': 'red', 'netral': 'orange'}\n",
    "plt.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%',\n",
    "        colors=[colors.get(x, 'gray') for x in sentiment_counts.index])\n",
    "plt.title('Distribusi Sentimen (Prediksi)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 3. Distribusi per Pondok Pesantren\n",
    "plt.subplot(3, 3, 3)\n",
    "ponpes_counts = sample_df['ponpes'].value_counts().head(10)\n",
    "plt.barh(range(len(ponpes_counts)), ponpes_counts.values, color='lightcoral')\n",
    "plt.title('Top 10 Pondok Pesantren (Jumlah Ulasan)', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Jumlah Ulasan')\n",
    "plt.yticks(range(len(ponpes_counts)), ponpes_counts.index)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# 4. Heatmap Aspek vs Sentimen\n",
    "plt.subplot(3, 3, 4)\n",
    "# Create crosstab\n",
    "aspect_sentiment_crosstab = pd.crosstab(sample_df['aspek_predicted'], sample_df['sentimen_predicted'])\n",
    "sns.heatmap(aspect_sentiment_crosstab, annot=True, fmt='d', cmap='YlOrRd')\n",
    "plt.title('Heatmap: Aspek vs Sentimen', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Sentimen')\n",
    "plt.ylabel('Aspek')\n",
    "\n",
    "# 5. Confusion Matrix - Aspek (jika ada data manual)\n",
    "if 'aspek_manual_encoded' in sample_df.columns:\n",
    "    plt.subplot(3, 3, 5)\n",
    "    sns.heatmap(aspect_cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - Aspek\\n(Accuracy: {aspect_accuracy:.3f})', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Prediksi')\n",
    "    plt.ylabel('Aktual')\n",
    "\n",
    "# 6. Confusion Matrix - Sentimen (jika ada data manual)\n",
    "if 'sentimen_lexicon' in sample_df.columns:\n",
    "    plt.subplot(3, 3, 6)\n",
    "    sns.heatmap(sentiment_cm, annot=True, fmt='d', cmap='Greens')\n",
    "    plt.title(f'Confusion Matrix - Sentimen\\n(Accuracy: {sentiment_accuracy:.3f})', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Prediksi')\n",
    "    plt.ylabel('Aktual')\n",
    "\n",
    "# 7. Distribusi Panjang Teks\n",
    "plt.subplot(3, 3, 7)\n",
    "sample_df['text_length'] = sample_df['text_refined'].astype(str).apply(len)\n",
    "plt.hist(sample_df['text_length'], bins=30, color='mediumpurple', alpha=0.7)\n",
    "plt.title('Distribusi Panjang Teks (Refined)', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Panjang Karakter')\n",
    "plt.ylabel('Frekuensi')\n",
    "\n",
    "# 8. Word Cloud untuk setiap sentimen (simulasi dengan bar chart kata populer)\n",
    "plt.subplot(3, 3, 8)\n",
    "# Get most common words for positive sentiment\n",
    "positive_texts = sample_df[sample_df['sentimen_predicted'] == 'positif']['text_refined']\n",
    "all_positive_words = ' '.join(positive_texts.astype(str)).lower().split()\n",
    "positive_word_freq = Counter([word for word in all_positive_words if len(word) > 3])\n",
    "top_positive_words = dict(positive_word_freq.most_common(10))\n",
    "\n",
    "if top_positive_words:\n",
    "    plt.bar(range(len(top_positive_words)), list(top_positive_words.values()), color='lightgreen')\n",
    "    plt.title('Kata Populer - Sentimen Positif', fontsize=12, fontweight='bold')\n",
    "    plt.xticks(range(len(top_positive_words)), list(top_positive_words.keys()), rotation=45, ha='right')\n",
    "    plt.ylabel('Frekuensi')\n",
    "\n",
    "# 9. Summary Statistics\n",
    "plt.subplot(3, 3, 9)\n",
    "# Create summary table\n",
    "summary_data = {\n",
    "    'Metrik': ['Total Data', 'Aspek Unik', 'Sentimen Positif', 'Sentimen Negatif', 'Sentimen Netral'],\n",
    "    'Nilai': [\n",
    "        len(sample_df),\n",
    "        sample_df['aspek_predicted'].nunique(),\n",
    "        len(sample_df[sample_df['sentimen_predicted'] == 'positif']),\n",
    "        len(sample_df[sample_df['sentimen_predicted'] == 'negatif']),\n",
    "        len(sample_df[sample_df['sentimen_predicted'] == 'netral'])\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Simple text-based summary\n",
    "plt.text(0.1, 0.7, f'RINGKASAN ANALISIS', fontsize=14, fontweight='bold')\n",
    "plt.text(0.1, 0.6, f'Total Data: {len(sample_df)}', fontsize=11)\n",
    "plt.text(0.1, 0.5, f'Aspek Unik: {sample_df[\"aspek_predicted\"].nunique()}', fontsize=11)\n",
    "plt.text(0.1, 0.4, f'Sentimen Positif: {len(sample_df[sample_df[\"sentimen_predicted\"] == \"positif\"])}', fontsize=11)\n",
    "plt.text(0.1, 0.3, f'Sentimen Negatif: {len(sample_df[sample_df[\"sentimen_predicted\"] == \"negatif\"])}', fontsize=11)\n",
    "plt.text(0.1, 0.2, f'Sentimen Netral: {len(sample_df[sample_df[\"sentimen_predicted\"] == \"netral\"])}', fontsize=11)\n",
    "\n",
    "if 'aspek_manual_encoded' in sample_df.columns:\n",
    "    plt.text(0.1, 0.1, f'Akurasi Aspek: {aspect_accuracy:.3f}', fontsize=11)\n",
    "if 'sentimen_lexicon' in sample_df.columns:\n",
    "    plt.text(0.1, 0.05, f'Akurasi Sentimen: {sentiment_accuracy:.3f}', fontsize=11)\n",
    "\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hasil_analisis_visualisasi.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002fc483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 8. MENYIMPAN HASIL\n",
    "# =======================\n",
    "print(\"\\n8. Menyimpan Hasil...\")\n",
    "\n",
    "# Prepare final results\n",
    "sample_df['aspek_label'] = sample_df['aspek_predicted'].map(aspect_labels)\n",
    "\n",
    "# Create final results dataframe\n",
    "hasil_final = sample_df[[\n",
    "    'no', 'ponpes', 'text_combined', 'text_refined',\n",
    "    'aspek_predicted', 'aspek_label', 'sentimen_predicted'\n",
    "]].copy()\n",
    "\n",
    "# Add manual labels if available\n",
    "if 'aspek_manual' in sample_df.columns:\n",
    "    hasil_final['aspek_manual'] = sample_df['aspek_manual']\n",
    "if 'sentimen_lexicon' in sample_df.columns:\n",
    "    hasil_final['sentimen_manual'] = sample_df['sentimen_lexicon']\n",
    "\n",
    "# Save results\n",
    "hasil_final.to_csv('hasil_analisis.csv', index=False)\n",
    "print(\"Hasil analisis berhasil disimpan ke 'hasil_analisis.csv'\")\n",
    "\n",
    "# Save model evaluation metrics\n",
    "evaluation_results = {\n",
    "    'total_data': len(sample_df),\n",
    "    'aspek_accuracy': aspect_accuracy if 'aspek_manual_encoded' in sample_df.columns else 'N/A',\n",
    "    'sentiment_accuracy': sentiment_accuracy if 'sentimen_lexicon' in sample_df.columns else 'N/A',\n",
    "    'aspek_distribution': sample_df['aspek_predicted'].value_counts().to_dict(),\n",
    "    'sentiment_distribution': sample_df['sentimen_predicted'].value_counts().to_dict()\n",
    "}\n",
    "\n",
    "# Save evaluation as JSON for further analysis\n",
    "import json\n",
    "with open('evaluation_metrics.json', 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Metrics evaluasi disimpan ke 'evaluation_metrics.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28caf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 9. RINGKASAN AKHIR\n",
    "# =======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RINGKASAN HASIL ANALISIS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total data diproses: {len(sample_df)}\")\n",
    "print(f\"Distribusi Aspek:\")\n",
    "for aspek_id, count in sample_df['aspek_predicted'].value_counts().sort_index().items():\n",
    "    print(f\"  {aspect_labels[aspek_id]}: {count}\")\n",
    "\n",
    "print(f\"\\nDistribusi Sentimen:\")\n",
    "for sentiment, count in sample_df['sentimen_predicted'].value_counts().items():\n",
    "    print(f\"  {sentiment.capitalize()}: {count}\")\n",
    "\n",
    "if 'aspek_manual_encoded' in sample_df.columns:\n",
    "    print(f\"\\nAkurasi Prediksi Aspek: {aspect_accuracy:.4f}\")\n",
    "if 'sentimen_lexicon' in sample_df.columns:\n",
    "    print(f\"Akurasi Prediksi Sentimen: {sentiment_accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nFile output:\")\n",
    "print(\"- hasil_analisis.csv: Hasil lengkap analisis\")\n",
    "print(\"- evaluation_metrics.json: Metrics evaluasi\")\n",
    "print(\"- hasil_analisis_visualisasi.png: Visualisasi hasil\")\n",
    "\n",
    "print(\"\\n=== ANALISIS SELESAI ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f96411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 10. SARAN OPTIMASI\n",
    "# =======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SARAN OPTIMASI DAN PENGEMBANGAN\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "saran_optimasi = \"\"\"\n",
    "1. TEXT REFINEMENT:\n",
    "   - Gunakan GPU untuk mempercepat inferensi mT5\n",
    "   - Pertimbangkan fine-tuning model mT5 pada domain pesantren\n",
    "   - Batch processing untuk efisiensi memori\n",
    "\n",
    "2. TOPIC MODELING:\n",
    "   - Eksperimen dengan jumlah topik berbeda (5-15)\n",
    "   - Gunakan coherence score untuk optimasi parameter\n",
    "   - Pertimbangkan BERTopic untuk hasil yang lebih baik\n",
    "\n",
    "3. SENTIMENT ANALYSIS:\n",
    "   - Perluas lexicon dengan kata-kata domain pesantren\n",
    "   - Gunakan word embedding untuk deteksi sinonim\n",
    "   - Pertimbangkan ensemble dengan model deep learning\n",
    "\n",
    "4. EVALUASI:\n",
    "   - Gunakan cross-validation untuk evaluasi yang robust\n",
    "   - Implementasikan metrics tambahan (F1-score, precision, recall)\n",
    "   - Analisis error untuk improvement\n",
    "\n",
    "5. VISUALISASI:\n",
    "   - Tambahkan interactive plots dengan Plotly\n",
    "   - Word cloud untuk setiap aspek\n",
    "   - Time series analysis jika ada data temporal\n",
    "\n",
    "6. SKALABILITAS:\n",
    "   - Implementasi parallel processing\n",
    "   - Database integration untuk dataset besar\n",
    "   - API endpoint untuk real-time prediction\n",
    "\"\"\"\n",
    "\n",
    "print(saran_optimasi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885c3cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMASI TAMBAHAN DAN FITUR LANJUTAN\n",
    "# Untuk meningkatkan performa dan akurasi analisis\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2340b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 1. OPTIMASI LDA DENGAN COHERENCE SCORE\n",
    "# =======================\n",
    "\n",
    "def optimize_lda_topics(texts, dictionary, corpus, topic_range=range(5, 16)):\n",
    "    \"\"\"Optimasi jumlah topik LDA menggunakan coherence score\"\"\"\n",
    "    print(\"Optimizing LDA topic numbers...\")\n",
    "    \n",
    "    coherence_scores = []\n",
    "    perplexity_scores = []\n",
    "    \n",
    "    for num_topics in tqdm(topic_range, desc=\"Testing topic numbers\"):\n",
    "        # Train LDA model\n",
    "        lda_model = models.LdaModel(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary,\n",
    "            num_topics=num_topics,\n",
    "            random_state=42,\n",
    "            passes=10,\n",
    "            alpha='auto'\n",
    "        )\n",
    "        \n",
    "        # Calculate coherence\n",
    "        coherence_model_lda = CoherenceModel(\n",
    "            model=lda_model,\n",
    "            texts=texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_v'\n",
    "        )\n",
    "        coherence_score = coherence_model_lda.get_coherence()\n",
    "        coherence_scores.append(coherence_score)\n",
    "        \n",
    "        # Calculate perplexity\n",
    "        perplexity = lda_model.log_perplexity(corpus)\n",
    "        perplexity_scores.append(perplexity)\n",
    "        \n",
    "        print(f\"Topics: {num_topics}, Coherence: {coherence_score:.4f}, Perplexity: {perplexity:.4f}\")\n",
    "    \n",
    "    # Find optimal number of topics\n",
    "    optimal_topics = topic_range[np.argmax(coherence_scores)]\n",
    "    \n",
    "    # Plot results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    ax1.plot(topic_range, coherence_scores, 'bo-')\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('Coherence Score')\n",
    "    ax1.set_title('Coherence Score vs Number of Topics')\n",
    "    ax1.axvline(x=optimal_topics, color='red', linestyle='--', label=f'Optimal: {optimal_topics}')\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.plot(topic_range, perplexity_scores, 'ro-')\n",
    "    ax2.set_xlabel('Number of Topics')\n",
    "    ax2.set_ylabel('Perplexity')\n",
    "    ax2.set_title('Perplexity vs Number of Topics')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lda_optimization.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return optimal_topics, coherence_scores, perplexity_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b0c2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 2. ENHANCED LEXICON SENTIMENT ANALYSIS\n",
    "# =======================\n",
    "\n",
    "class EnhancedSentimentAnalyzer:\n",
    "    def __init__(self, positive_words, negative_words):\n",
    "        self.positive_words = set(positive_words)\n",
    "        self.negative_words = set(negative_words)\n",
    "        \n",
    "        # Tambahan kata-kata khusus domain pesantren\n",
    "        self.pesantren_positive = {\n",
    "            'baik', 'bagus', 'hebat', 'luar biasa', 'excellent', 'terbaik',\n",
    "            'berkualitas', 'memuaskan', 'sempurna', 'istimewa', 'mantap',\n",
    "            'berkah', 'barokah', 'mulia', 'terpuji', 'amanah', 'sholeh',\n",
    "            'islami', 'religius', 'spiritual', 'mendidik', 'bermanfaat'\n",
    "        }\n",
    "        \n",
    "        self.pesantren_negative = {\n",
    "            'buruk', 'jelek', 'tidak baik', 'mengecewakan', 'kurang',\n",
    "            'minim', 'terbatas', 'kotor', 'rusak', 'tidak terawat',\n",
    "            'tidak disiplin', 'tidak tertib', 'chaos', 'berantakan'\n",
    "        }\n",
    "        \n",
    "        # Gabungkan dengan lexicon utama\n",
    "        self.positive_words.update(self.pesantren_positive)\n",
    "        self.negative_words.update(self.pesantren_negative)\n",
    "        \n",
    "        # Weight untuk kata-kata tertentu\n",
    "        self.positive_weights = defaultdict(lambda: 1)\n",
    "        self.negative_weights = defaultdict(lambda: 1)\n",
    "        \n",
    "        # Kata-kata dengan bobot tinggi\n",
    "        high_weight_positive = ['excellent', 'luar biasa', 'terbaik', 'sempurna', 'istimewa']\n",
    "        high_weight_negative = ['buruk', 'mengecewakan', 'tidak baik', 'jelek']\n",
    "        \n",
    "        for word in high_weight_positive:\n",
    "            self.positive_weights[word] = 2\n",
    "        for word in high_weight_negative:\n",
    "            self.negative_weights[word] = 2\n",
    "    \n",
    "    def analyze_sentiment_enhanced(self, text):\n",
    "        \"\"\"Enhanced sentiment analysis dengan bobot dan konteks\"\"\"\n",
    "        if pd.isna(text) or text == \"\":\n",
    "            return 'netral', 0.0\n",
    "        \n",
    "        tokens = word_tokenize(text.lower())\n",
    "        \n",
    "        # Hitung skor dengan bobot\n",
    "        pos_score = sum(self.positive_weights[word] for word in tokens if word in self.positive_words)\n",
    "        neg_score = sum(self.negative_weights[word] for word in tokens if word in self.negative_words)\n",
    "        \n",
    "        # Normalisasi berdasarkan panjang teks\n",
    "        text_length = len(tokens)\n",
    "        if text_length > 0:\n",
    "            pos_score = pos_score / text_length\n",
    "            neg_score = neg_score / text_length\n",
    "        \n",
    "        # Tentukan sentimen dengan confidence score\n",
    "        if pos_score > neg_score:\n",
    "            confidence = pos_score / (pos_score + neg_score) if (pos_score + neg_score) > 0 else 0\n",
    "            return 'positif', confidence\n",
    "        elif neg_score > pos_score:\n",
    "            confidence = neg_score / (pos_score + neg_score) if (pos_score + neg_score) > 0 else 0\n",
    "            return 'negatif', confidence\n",
    "        else:\n",
    "            return 'netral', 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19a7f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 3. PARALLEL PROCESSING UNTUK TEXT REFINEMENT\n",
    "# =======================\n",
    "\n",
    "def process_text_batch(texts_batch, refiner):\n",
    "    \"\"\"Process batch of texts in parallel\"\"\"\n",
    "    results = []\n",
    "    for text in texts_batch:\n",
    "        refined_text = refiner.refine_text(text)\n",
    "        results.append(refined_text)\n",
    "    return results\n",
    "\n",
    "def parallel_text_refinement(texts, refiner, n_processes=None):\n",
    "    \"\"\"Parallel processing untuk text refinement\"\"\"\n",
    "    if n_processes is None:\n",
    "        n_processes = min(mp.cpu_count(), 4)  # Limit to 4 processes\n",
    "    \n",
    "    # Split texts into batches\n",
    "    batch_size = max(1, len(texts) // n_processes)\n",
    "    text_batches = [texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]\n",
    "    \n",
    "    print(f\"Processing {len(texts)} texts with {n_processes} processes...\")\n",
    "    \n",
    "    # Process in parallel\n",
    "    with mp.Pool(n_processes) as pool:\n",
    "        batch_results = pool.map(partial(process_text_batch, refiner=refiner), text_batches)\n",
    "    \n",
    "    # Flatten results\n",
    "    refined_texts = []\n",
    "    for batch_result in batch_results:\n",
    "        refined_texts.extend(batch_result)\n",
    "    \n",
    "    return refined_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aef798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 4. INTERACTIVE VISUALIZATIONS dengan PLOTLY\n",
    "# =======================\n",
    "\n",
    "def create_interactive_visualizations(df):\n",
    "    \"\"\"Membuat visualisasi interaktif dengan Plotly\"\"\"\n",
    "    \n",
    "    # 1. Interactive Aspect Distribution\n",
    "    fig1 = px.bar(\n",
    "        x=df['aspek_predicted'].value_counts().index,\n",
    "        y=df['aspek_predicted'].value_counts().values,\n",
    "        title=\"Distribusi Aspek (Interactive)\",\n",
    "        labels={'x': 'Aspek', 'y': 'Jumlah'},\n",
    "        color=df['aspek_predicted'].value_counts().values,\n",
    "        color_continuous_scale='viridis'\n",
    "    )\n",
    "    fig1.update_xaxis(title=\"Aspek\")\n",
    "    fig1.update_yaxis(title=\"Jumlah\")\n",
    "    fig1.write_html(\"distribusi_aspek_interactive.html\")\n",
    "    \n",
    "    # 2. Interactive Sentiment Pie Chart\n",
    "    sentiment_counts = df['sentimen_predicted'].value_counts()\n",
    "    fig2 = px.pie(\n",
    "        values=sentiment_counts.values,\n",
    "        names=sentiment_counts.index,\n",
    "        title=\"Distribusi Sentimen (Interactive)\",\n",
    "        color_discrete_map={\n",
    "            'positif': '#2E8B57',\n",
    "            'negatif': '#DC143C',\n",
    "            'netral': '#FF8C00'\n",
    "        }\n",
    "    )\n",
    "    fig2.write_html(\"distribusi_sentimen_interactive.html\")\n",
    "    \n",
    "    # 3. Interactive Heatmap\n",
    "    aspect_sentiment_crosstab = pd.crosstab(df['aspek_predicted'], df['sentimen_predicted'])\n",
    "    fig3 = px.imshow(\n",
    "        aspect_sentiment_crosstab.values,\n",
    "        labels=dict(x=\"Sentimen\", y=\"Aspek\", color=\"Jumlah\"),\n",
    "        x=aspect_sentiment_crosstab.columns,\n",
    "        y=aspect_sentiment_crosstab.index,\n",
    "        title=\"Heatmap: Aspek vs Sentimen (Interactive)\",\n",
    "        color_continuous_scale='RdYlBu'\n",
    "    )\n",
    "    fig3.write_html(\"heatmap_aspek_sentimen_interactive.html\")\n",
    "    \n",
    "    # 4. Multi-dimensional Analysis\n",
    "    fig4 = px.scatter(\n",
    "        df, \n",
    "        x='aspek_predicted', \n",
    "        y='sentimen_predicted',\n",
    "        size='text_length' if 'text_length' in df.columns else None,\n",
    "        color='ponpes',\n",
    "        title=\"Analisis Multi-dimensi: Aspek vs Sentimen per Pesantren\",\n",
    "        hover_data=['ponpes', 'text_combined']\n",
    "    )\n",
    "    fig4.write_html(\"analisis_multidimensi_interactive.html\")\n",
    "    \n",
    "    print(\"Interactive visualizations saved:\")\n",
    "    print(\"- distribusi_aspek_interactive.html\")\n",
    "    print(\"- distribusi_sentimen_interactive.html\") \n",
    "    print(\"- heatmap_aspek_sentimen_interactive.html\")\n",
    "    print(\"- analisis_multidimensi_interactive.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a6793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 5. ADVANCED EVALUATION METRICS\n",
    "# =======================\n",
    "\n",
    "def comprehensive_evaluation(y_true, y_pred, labels=None):\n",
    "    \"\"\"Evaluasi komprehensif dengan berbagai metrics\"\"\"\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Basic metrics\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    metrics['f1_weighted'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics['precision_macro'] = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    metrics['recall_macro'] = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    # Per-class metrics\n",
    "    if labels is not None:\n",
    "        f1_per_class = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "        precision_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "        recall_per_class = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "        \n",
    "        for i, label in enumerate(labels):\n",
    "            if i < len(f1_per_class):\n",
    "                metrics[f'f1_{label}'] = f1_per_class[i]\n",
    "                metrics[f'precision_{label}'] = precision_per_class[i]\n",
    "                metrics[f'recall_{label}'] = recall_per_class[i]\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def cross_validation_evaluation(X, y, model, cv=5):\n",
    "    \"\"\"Cross-validation evaluation\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
    "    \n",
    "    return {\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'cv_scores': cv_scores.tolist()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e7bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 6. AUTOMATED REPORT GENERATION\n",
    "# =======================\n",
    "\n",
    "def generate_analysis_report(df, metrics_dict, output_file='analisis_report.html'):\n",
    "    \"\"\"Generate comprehensive HTML report\"\"\"\n",
    "    \n",
    "    html_content = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Laporan Analisis Sentimen dan Aspek Pesantren</title>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; margin: 40px; }}\n",
    "            .header {{ background-color: #2E8B57; color: white; padding: 20px; text-align: center; }}\n",
    "            .section {{ margin: 20px 0; padding: 15px; border-left: 4px solid #2E8B57; }}\n",
    "            .metric {{ background-color: #f8f9fa; padding: 10px; margin: 5px 0; border-radius: 5px; }}\n",
    "            table {{ border-collapse: collapse; width: 100%; }}\n",
    "            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
    "            th {{ background-color: #f2f2f2; }}\n",
    "            .positive {{ color: #2E8B57; font-weight: bold; }}\n",
    "            .negative {{ color: #DC143C; font-weight: bold; }}\n",
    "            .neutral {{ color: #FF8C00; font-weight: bold; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"header\">\n",
    "            <h1>Laporan Analisis Sentimen dan Aspek Ulasan Pesantren</h1>\n",
    "            <p>Generated on: {time.strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"section\">\n",
    "            <h2>Ringkasan Dataset</h2>\n",
    "            <div class=\"metric\">Total Data: {len(df)}</div>\n",
    "            <div class=\"metric\">Jumlah Pesantren: {df['ponpes'].nunique()}</div>\n",
    "            <div class=\"metric\">Rata-rata Panjang Teks: {df['text_refined'].astype(str).apply(len).mean():.1f} karakter</div>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"section\">\n",
    "            <h2>Distribusi Sentimen</h2>\n",
    "            <table>\n",
    "                <tr><th>Sentimen</th><th>Jumlah</th><th>Persentase</th></tr>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add sentiment distribution\n",
    "    sentiment_counts = df['sentimen_predicted'].value_counts()\n",
    "    for sentiment, count in sentiment_counts.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        css_class = sentiment.lower() if sentiment.lower() in ['positive', 'negative'] else 'neutral'\n",
    "        html_content += f'<tr><td class=\"{css_class}\">{sentiment.capitalize()}</td><td>{count}</td><td>{percentage:.1f}%</td></tr>'\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "            </table>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"section\">\n",
    "            <h2>Distribusi Aspek</h2>\n",
    "            <table>\n",
    "                <tr><th>ID</th><th>Aspek</th><th>Jumlah</th><th>Persentase</th></tr>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add aspect distribution\n",
    "    aspect_labels = {\n",
    "        0: \"Kualitas Guru\", 1: \"Fasilitas\", 2: \"Lingkungan\", 3: \"Kegiatan Pondok\",\n",
    "        4: \"Pembinaan Karakter\", 5: \"Prestasi\", 6: \"Akademik\", 7: \"Motivasi/Spiritual\",\n",
    "        8: \"Sosial\", 9: \"Umum\"\n",
    "    }\n",
    "    \n",
    "    aspect_counts = df['aspek_predicted'].value_counts().sort_index()\n",
    "    for aspect_id, count in aspect_counts.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        aspect_name = aspect_labels.get(aspect_id, f\"Aspek {aspect_id}\")\n",
    "        html_content += f'<tr><td>{aspect_id}</td><td>{aspect_name}</td><td>{count}</td><td>{percentage:.1f}%</td></tr>'\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "            </table>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"section\">\n",
    "            <h2>Metrics Evaluasi</h2>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add evaluation metrics if available\n",
    "    for metric_name, metric_value in metrics_dict.items():\n",
    "        if isinstance(metric_value, float):\n",
    "            html_content += f'<div class=\"metric\">{metric_name}: {metric_value:.4f}</div>'\n",
    "        else:\n",
    "            html_content += f'<div class=\"metric\">{metric_name}: {metric_value}</div>'\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"section\">\n",
    "            <h2>Top Pesantren (Berdasarkan Jumlah Ulasan)</h2>\n",
    "            <table>\n",
    "                <tr><th>Nama Pesantren</th><th>Jumlah Ulasan</th></tr>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add top pesantren\n",
    "    top_pesantren = df['ponpes'].value_counts().head(10)\n",
    "    for pesantren, count in top_pesantren.items():\n",
    "        html_content += f'<tr><td>{pesantren}</td><td>{count}</td></tr>'\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "            </table>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"section\">\n",
    "            <h2>Rekomendasi</h2>\n",
    "            <ul>\n",
    "                <li>Lakukan monitoring berkala untuk tren sentimen</li>\n",
    "                <li>Fokus pada aspek dengan sentimen negatif tinggi</li>\n",
    "                <li>Tingkatkan kualitas pada aspek yang sering dikomentari</li>\n",
    "                <li>Manfaatkan feedback positif untuk promosi</li>\n",
    "            </ul>\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save report\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    print(f\"Comprehensive report saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b9a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 7. MAIN EXECUTION FUNCTION\n",
    "# =======================\n",
    "\n",
    "def run_enhanced_analysis(df, positive_words, negative_words):\n",
    "    \"\"\"Run enhanced analysis with all optimizations\"\"\"\n",
    "    \n",
    "    print(\"=== ENHANCED ANALYSIS STARTED ===\\n\")\n",
    "    \n",
    "    # 1. Enhanced Sentiment Analysis\n",
    "    print(\"1. Running Enhanced Sentiment Analysis...\")\n",
    "    enhanced_analyzer = EnhancedSentimentAnalyzer(positive_words, negative_words)\n",
    "    \n",
    "    sentiment_results = df['text_refined'].apply(enhanced_analyzer.analyze_sentiment_enhanced)\n",
    "    df['sentimen_enhanced'] = [result[0] for result in sentiment_results]\n",
    "    df['sentiment_confidence'] = [result[1] for result in sentiment_results]\n",
    "    \n",
    "    # 2. Create Interactive Visualizations\n",
    "    print(\"2. Creating Interactive Visualizations...\")\n",
    "    create_interactive_visualizations(df)\n",
    "    \n",
    "    # 3. Comprehensive Evaluation\n",
    "    print(\"3. Running Comprehensive Evaluation...\")\n",
    "    evaluation_metrics = {}\n",
    "    \n",
    "    if 'sentimen_lexicon' in df.columns:\n",
    "        sentiment_metrics = comprehensive_evaluation(\n",
    "            df['sentimen_lexicon'], \n",
    "            df['sentimen_enhanced'],\n",
    "            labels=['positif', 'negatif', 'netral']\n",
    "        )\n",
    "        evaluation_metrics.update({f'sentiment_{k}': v for k, v in sentiment_metrics.items()})\n",
    "    \n",
    "    if 'aspek_manual' in df.columns:\n",
    "        # Convert to numeric if needed\n",
    "        if df['aspek_manual'].dtype == 'object':\n",
    "            from sklearn.preprocessing import LabelEncoder\n",
    "            le = LabelEncoder()\n",
    "            aspek_manual_numeric = le.fit_transform(df['aspek_manual'].astype(str))\n",
    "        else:\n",
    "            aspek_manual_numeric = df['aspek_manual']\n",
    "        \n",
    "        aspect_metrics = comprehensive_evaluation(\n",
    "            aspek_manual_numeric,\n",
    "            df['aspek_predicted'],\n",
    "            labels=list(range(10))\n",
    "        )\n",
    "        evaluation_metrics.update({f'aspect_{k}': v for k, v in aspect_metrics.items()})\n",
    "    \n",
    "    # 4. Generate Comprehensive Report\n",
    "    print(\"4. Generating Comprehensive Report...\")\n",
    "    generate_analysis_report(df, evaluation_metrics)\n",
    "    \n",
    "    # 5. Save Enhanced Results\n",
    "    print(\"5. Saving Enhanced Results...\")\n",
    "    df.to_csv('hasil_analisis_enhanced.csv', index=False)\n",
    "    \n",
    "    # Save metrics\n",
    "    import json\n",
    "    with open('enhanced_metrics.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(evaluation_metrics, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"\\n=== ENHANCED ANALYSIS COMPLETED ===\")\n",
    "    print(\"Output files:\")\n",
    "    print(\"- hasil_analisis_enhanced.csv\")\n",
    "    print(\"- enhanced_metrics.json\") \n",
    "    print(\"- analisis_report.html\")\n",
    "    print(\"- Interactive HTML visualizations\")\n",
    "    \n",
    "    return df, evaluation_metrics\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Load your data\n",
    "#     df = pd.read_csv('hasil_sentimen_pesantren.csv')\n",
    "#     positive_words = pd.read_csv('positive.csv')['word'].tolist()\n",
    "#     negative_words = pd.read_csv('negative.csv')['word'].tolist()\n",
    "#     \n",
    "#     # Run enhanced analysis\n",
    "#     enhanced_df, metrics = run_enhanced_analysis(df, positive_words, negative_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "absa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
